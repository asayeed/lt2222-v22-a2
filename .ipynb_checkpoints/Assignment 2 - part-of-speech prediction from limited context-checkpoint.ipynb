{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4045a47f",
   "metadata": {},
   "source": [
    "# Assignment 2 - part-of-speech prediction from limited context\n",
    "\n",
    "In this assignment, you will train classifiers that attempt, within a window of five words, to make a binary prediction about whether the third word belongs to a given part of speech (noun, verb, adjective, adverb), but using very limited information -- that is, the last two letters of the first, second, fourth, and fifth word of the sequence, and no information whatsoever directly from the third word itself.  You will strip out all punctuation (using the NLTK `WordPunctTokenizer`), lowercase, and remove stop words (using the NLTK English stop words list).\n",
    "\n",
    "In other words, you will predict over samples that have two classes, P and not-P, where P is the selected part of speech to classify.  For example, from the sentence, \"The quick brown fox jumped over the lazy dog.\", we can select the following 5-word windows without stop words, \"brown fox jumped lazy dog\" and \"quick brown fox jumped lazy\".  If we select verbs as the part-of-speech we are classifying over, we get the instances <(wn,ox,zy,og),1>, since \"jumped\" is a verb, but <(ck,wn,ed,zy),0> because \"fox\" in that context is not.\n",
    "\n",
    "This means that you will need to take into account the position of the last-two-letter feature:  \"zy\" as the fourth word's last two letters is different from \"zy\" as the fifth word's last two letters.  They are two features, say, `zy_4` and `zy_5`.\n",
    "\n",
    "This will likely not actually work.  But it might!\n",
    "\n",
    "You will create training and testing samples according to this procedure, and you will build a data structure that can be fed to a support vector machine (SVM) classifier.  You will train the classifier on the training data and evaluate it on the testing data. \n",
    "\n",
    "The work will be done in a .py module file in the same folder as this notebook.  **No modifications to this notebook will be graded.** We will run your module using this notebook or one we modify that you won't see in order to test your code.\n",
    "\n",
    "The file you must create and add to the github repo is `mycode.py`, which will be imported here.  You can create your own notebooks or scripts to test it.  You can put any number of your own helper functions and also put optional parameters on any of the python functions mentioned here. You should also create a Markdown file, `notes.md`, to keep any **concise** notes and remarks about the assignment.  The code must run on mltgpu.\n",
    "\n",
    "**This assignment is due Monday, 2022 March 7, at 23:59. There are 33 points and 5 bonus points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6be47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61facb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mycode as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16dc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca marche!\n"
     ]
    }
   ],
   "source": [
    "mc.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e40ca",
   "metadata": {},
   "source": [
    "## Part 0 - preparation (2 points)\n",
    "\n",
    "Fork this repository and create and add `mycode.py` and `notes.md`. \n",
    "\n",
    "## Part 1 - obtaining the text (3 points)\n",
    "\n",
    "You will randomly select the given number of lines from the gzipped file we give you (so you will have to figure out how to access gzipped text files).  Explain how you implemented the random selection in `notes.md`. When we run it, it should give a new sample every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612f08f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'(Thirty-fourth session, Geneva, 18-20 September 2002)\\n',\n",
       " b'Mr. Walid Al-Hadid\\n',\n",
       " b'It was also considered unacceptable for public funds to be used to compensate for loss that should be allocated to the operator.\\n',\n",
       " b'INLAND TRANSPORT COMMITTEE\\n',\n",
       " b'Mr. J. F. Kissack\\n',\n",
       " b'Those issues and the needed response were highlighted in the Almaty Programme of Action.\\n',\n",
       " b'We are dealing with the agenda item entitled \"Oceans and the law of the sea\" on the very same day we are celebrating the tenth anniversary of the entry into force of the United Nations Convention on the Law of the Sea, a text of historic impact. The Convention has made an indisputable contribution to the codification of international law of the sea and is an important milestone in the establishment of a global legal framework for governance of the various marine areas and their living and non-living resources.\\n',\n",
       " b'The latter would continue to be reviewed by UNMOVIC and IAEA to ensure that items were not included in the goods review list or otherwise subject to paragraph 24 of resolution 687 (1991).\\n',\n",
       " b'The Government had placed these issues high on its agenda and had sought new measures tailored to the needs of modern society.\\n',\n",
       " b'(b) Guatemalan Social Security Institute (IGSS) (2 per cent):\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sampled_lines = mc.sample_lines(\"/scratch/UN-english.txt.gz\", lines=100000)\n",
    "\n",
    "print(len(sampled_lines))\n",
    "sampled_lines[40000:40010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94a339",
   "metadata": {},
   "source": [
    "## Part 2 - creating the samples (7 points)\n",
    "\n",
    "From the sampled lines, you will then randomly create the five-word samples.\n",
    "\n",
    "You will tokenize the sentences and apply POS-tagging to them -- you need to do this before you create the samples, since POS-tagging needs context. You will then remove stop words and punctuation and lowercase the remainder.  Next, you will randomly, over the entire set of sentences, choose samples of five words in sequence, up to a certain limit.  You find the last two characters of the first, second, fourth, and fifth words, and create the type of structure specified up in the introduction to this assignment for each sample. The exact representation is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e29931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "def list_df_POStag(s):\n",
    "    list_of_df=[]\n",
    "    for sent in s:\n",
    "        \n",
    "        list_of_df.append(pd.DataFrame(nltk.pos_tag(WordPunctTokenizer().tokenize(str(sent).strip(\"b\\\" ,()f, .:;'\\\\\").removesuffix(\"\\\\n\").strip(\",. ;:'\\\"'\"))),columns=['word','POS']))\n",
    "        \n",
    "    return list_of_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "64854521",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df=list_df_POStag(sampled_lines[40000:40010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3cada66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[             word  POS\n",
       " 0              In   IN\n",
       " 1            this   DT\n",
       " 2         context   NN\n",
       " 3               ,    ,\n",
       " 4             the   DT\n",
       " 5       Committee  NNP\n",
       " 6             has  VBZ\n",
       " 7        likewise   RB\n",
       " 8         focused  VBN\n",
       " 9              on   IN\n",
       " 10            the   DT\n",
       " 11  participation   NN\n",
       " 12             of   IN\n",
       " 13          rural   JJ\n",
       " 14          women  NNS\n",
       " 15             in   IN\n",
       " 16          local   JJ\n",
       " 17            and   CC\n",
       " 18       national   JJ\n",
       " 19         public   JJ\n",
       " 20       decision   NN\n",
       " 21              -    :\n",
       " 22         making   NN\n",
       " 23              ,    ,\n",
       " 24             as   IN\n",
       " 25              a   DT\n",
       " 26          means   NN\n",
       " 27             of   IN\n",
       " 28    empowerment   NN\n",
       " 29            and   CC\n",
       " 30             of   IN\n",
       " 31      enhancing  VBG\n",
       " 32         access   NN\n",
       " 33             to   TO\n",
       " 34     productive   JJ\n",
       " 35      resources  NNS,\n",
       "             word  POS\n",
       " 0            The   DT\n",
       " 1           list   NN\n",
       " 2             of   IN\n",
       " 3     additional   JJ\n",
       " 4   agricultural   JJ\n",
       " 5          items  NNS\n",
       " 6            was  VBD\n",
       " 7       approved  VBN\n",
       " 8             on   IN\n",
       " 9             23   CD\n",
       " 10      February  NNP\n",
       " 11          2001   CD\n",
       " 12           and   CC\n",
       " 13           the   DT\n",
       " 14          list   NN\n",
       " 15            of   IN\n",
       " 16    additional   JJ\n",
       " 17         items  NNS\n",
       " 18            in   IN\n",
       " 19           the   DT\n",
       " 20        health   NN\n",
       " 21        sector   NN\n",
       " 22           was  VBD\n",
       " 23      approved  VBN\n",
       " 24            on   IN\n",
       " 25            27   CD\n",
       " 26      February  NNP\n",
       " 27          2001   CD,\n",
       "            word   POS\n",
       " 0            In    IN\n",
       " 1          that    DT\n",
       " 2        spirit    NN\n",
       " 3             ,     ,\n",
       " 4           the    DT\n",
       " 5         Crown   NNP\n",
       " 6           had   VBD\n",
       " 7       offered   VBN\n",
       " 8           its  PRP$\n",
       " 9     apologies   NNS\n",
       " 10           to    TO\n",
       " 11            a    DT\n",
       " 12       number    NN\n",
       " 13           of    IN\n",
       " 14  communities   NNS\n",
       " 15         that   WDT\n",
       " 16          had   VBD\n",
       " 17            ,     ,\n",
       " 18           at    IN\n",
       " 19      various    JJ\n",
       " 20        times   NNS\n",
       " 21            ,     ,\n",
       " 22     suffered   VBD\n",
       " 23    injustice    NN\n",
       " 24           at    IN\n",
       " 25          the    DT\n",
       " 26        hands   NNS\n",
       " 27           of    IN\n",
       " 28          the    DT\n",
       " 29          New   NNP\n",
       " 30      Zealand   NNP\n",
       " 31   Government   NNP,\n",
       "         word  POS\n",
       " 0          d   NN\n",
       " 1          )    )\n",
       " 2        The   DT\n",
       " 3   premises  NNS\n",
       " 4         of   IN\n",
       " 5        the   DT\n",
       " 6      Court  NNP\n",
       " 7      shall   MD\n",
       " 8        not   RB\n",
       " 9     become   VB\n",
       " 10         a   DT\n",
       " 11    refuge   NN\n",
       " 12      from   IN\n",
       " 13   justice   NN,\n",
       "             word  POS\n",
       " 0        Special   JJ\n",
       " 1     Rapporteur  NNP\n",
       " 2             on   IN\n",
       " 3            the   DT\n",
       " 4           sale   NN\n",
       " 5             of   IN\n",
       " 6       children  NNS\n",
       " 7              ,    ,\n",
       " 8          child   NN\n",
       " 9   prostitution   NN\n",
       " 10           and   CC\n",
       " 11         child   NN\n",
       " 12   pornography   NN,\n",
       "          word  POS\n",
       " 0          Mr  NNP\n",
       " 1           .    .\n",
       " 2      Nisuke  NNP\n",
       " 3        Ando  NNP\n",
       " 4           ,    ,\n",
       " 5          Ms  NNP\n",
       " 6           .    .\n",
       " 7   Christine  NNP\n",
       " 8      Chanet  NNP\n",
       " 9           ,    ,\n",
       " 10         Mr  NNP\n",
       " 11          .    .\n",
       " 12     Eckart  NNP\n",
       " 13      Klein  NNP,\n",
       "                word  POS\n",
       " 0          Likewise   RB\n",
       " 1                 ,    ,\n",
       " 2               the   DT\n",
       " 3           sharing   NN\n",
       " 4                of   IN\n",
       " 5                 \"  NNP\n",
       " 6            render   NN\n",
       " 7                 -    :\n",
       " 8              safe   JJ\n",
       " 9                 \"   NN\n",
       " 10       procedures  NNS\n",
       " 11              may   MD\n",
       " 12           reveal   VB\n",
       " 13        sensitive   JJ\n",
       " 14        technical   JJ\n",
       " 15  characteristics  NNS\n",
       " 16            about   IN\n",
       " 17          weapons  NNS\n",
       " 18              and   CC\n",
       " 19              may   MD\n",
       " 20        therefore   VB\n",
       " 21              not   RB\n",
       " 22               be   VB\n",
       " 23          readily   RB\n",
       " 24       releasable   JJ,\n",
       "           word  POS\n",
       " 0            1   CD\n",
       " 1            .    .\n",
       " 2           In   IN\n",
       " 3          the   DT\n",
       " 4     northern   JJ\n",
       " 5       region   NN\n",
       " 6          130   CD\n",
       " 7      sorties  NNS\n",
       " 8         were  VBD\n",
       " 9        flown  VBN\n",
       " 10          at   IN\n",
       " 11      speeds  NNS\n",
       " 12          of   IN\n",
       " 13         720   CD\n",
       " 14          to   TO\n",
       " 15         780   CD\n",
       " 16  kilometres  NNS\n",
       " 17         per   IN\n",
       " 18        hour   NN\n",
       " 19         and   CC\n",
       " 20          at   IN\n",
       " 21   altitudes  NNS\n",
       " 22          of   IN\n",
       " 23           6   CD\n",
       " 24           ,    ,\n",
       " 25         000   CD\n",
       " 26          to   TO\n",
       " 27          12   CD\n",
       " 28           ,    ,\n",
       " 29         000   CD\n",
       " 30      metres  NNS\n",
       " 31           ,    ,\n",
       " 32          as   IN\n",
       " 33     follows  VBZ,\n",
       "            word   POS\n",
       " 0            16    CD\n",
       " 1             .     .\n",
       " 2         Since    IN\n",
       " 3          many    JJ\n",
       " 4            of    IN\n",
       " 5       society    NN\n",
       " 6             '   POS\n",
       " 7             s    NN\n",
       " 8      problems   NNS\n",
       " 9         could    MD\n",
       " 10          not    RB\n",
       " 11           be    VB\n",
       " 12       solved   VBN\n",
       " 13           in    IN\n",
       " 14    isolation    NN\n",
       " 15            ,     ,\n",
       " 16        civil    JJ\n",
       " 17      society    NN\n",
       " 18       played   VBD\n",
       " 19            a    DT\n",
       " 20      crucial    JJ\n",
       " 21         role    NN\n",
       " 22          and    CC\n",
       " 23         must    MD\n",
       " 24    cooperate    VB\n",
       " 25         with    IN\n",
       " 26          the    DT\n",
       " 27       United   NNP\n",
       " 28      Nations  NNPS\n",
       " 29            ,     ,\n",
       " 30         with    IN\n",
       " 31          the    DT\n",
       " 32     approval    NN\n",
       " 33          and    CC\n",
       " 34      support    NN\n",
       " 35           of    IN\n",
       " 36         host    NN\n",
       " 37  Governments   NNS,\n",
       "          word   POS\n",
       " 0         The    DT\n",
       " 1       Group   NNP\n",
       " 2         was   VBD\n",
       " 3     briefed   VBN\n",
       " 4          on    IN\n",
       " 5         the    DT\n",
       " 6       quota    NN\n",
       " 7      system    NN\n",
       " 8        that   WDT\n",
       " 9         had   VBD\n",
       " 10       been   VBN\n",
       " 11  developed   VBN\n",
       " 12        for    IN\n",
       " 13        the    DT\n",
       " 14     number    NN\n",
       " 15         of    IN\n",
       " 16   pilgrims   NNS\n",
       " 17       from    IN\n",
       " 18       each    DT\n",
       " 19    country    NN\n",
       " 20        and    CC\n",
       " 21        its  PRP$\n",
       " 22  operation    NN]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5919699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg=list_df_POStag([\"that happens that the word  67 happens is said twice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60a07ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happens</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happens</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twice</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  POS\n",
       "0     that   DT\n",
       "1  happens  VBZ\n",
       "2     that   IN\n",
       "3      the   DT\n",
       "4     word   NN\n",
       "5       67   CD\n",
       "6  happens  NNS\n",
       "7       is  VBZ\n",
       "8     said  VBD\n",
       "9    twice   RB"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "449e4f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word POS\n",
       "0  that  DT\n",
       "2  that  IN"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0][dg[0]['word']=='that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb0ea49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       that\n",
       "1    happens\n",
       "2       that\n",
       "3        the\n",
       "4       word\n",
       "5         67\n",
       "6    happens\n",
       "7         is\n",
       "8       said\n",
       "9      twice\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0]['word'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9a12fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happens</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happens</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twice</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  POS\n",
       "0     that   DT\n",
       "1  happens  VBZ\n",
       "2     that   IN\n",
       "3      the   DT\n",
       "4     word   NN\n",
       "6  happens  NNS\n",
       "7       is  VBZ\n",
       "8     said  VBD\n",
       "9    twice   RB"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0].drop(dg[0][~dg[0]['word'].str.isalpha()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14b91332",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "pattern = '|'.join(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d53bb90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2     True\n",
       "3    False\n",
       "4     True\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8    False\n",
       "9    False\n",
       "Name: word, dtype: bool"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0]['word'].str.contains(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54155c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "def list_dic_POStag1(s):\n",
    "    list_of_dic=[]\n",
    "    for sent in s:\n",
    "        \n",
    "        list_of_dic.append(pd.DataFrame(nltk.pos_tag(WordPunctTokenizer().tokenize(str(sent).strip(\"b\\\" ,()f, .:;'\\\\\").removesuffix(\"\\\\n\").strip(\",. ;:'\\\"'\"))),columns=['word','POS']))\n",
    "        \n",
    "    return list_of_dic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "557e7e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happens</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happens</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twice</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  POS\n",
       "1  happens  VBZ\n",
       "3      the   DT\n",
       "5       67   CD\n",
       "6  happens  NNS\n",
       "7       is  VBZ\n",
       "8     said  VBD\n",
       "9    twice   RB"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern='|'.join(['that','word'])\n",
    "dg[0].drop(dg[0][dg[0]['word'].str.contains(pattern)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d0d227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           word  POS\n",
       " 0             d   NN\n",
       " 1             )    )\n",
       " 2  Proportional  NNP\n",
       " 3         means  NNS,\n",
       "          word  POS\n",
       " 0       Voice  NNP\n",
       " 1   reporting  VBG\n",
       " 2         via   IN\n",
       " 3         VHF  NNP\n",
       " 4           (    (\n",
       " 5          ch   NN\n",
       " 6           .    .\n",
       " 7           5   CD\n",
       " 8           .    .\n",
       " 9           1   CD\n",
       " 10          .    .\n",
       " 11          2   CD\n",
       " 12          (    (\n",
       " 13          7   CD\n",
       " 14         ))   NN,\n",
       "           word   POS\n",
       " 0    Therefore    RB\n",
       " 1            ,     ,\n",
       " 2          the    DT\n",
       " 3       author    NN\n",
       " 4       argues   VBZ\n",
       " 5         that    RB\n",
       " 6            ,     ,\n",
       " 7           as    IN\n",
       " 8           he   PRP\n",
       " 9          had   VBD\n",
       " 10          at    IN\n",
       " 11       least   JJS\n",
       " 12          up    RB\n",
       " 13       until    IN\n",
       " 14           5    CD\n",
       " 15     January   NNP\n",
       " 16        1999    CD\n",
       " 17           (     (\n",
       " 18         two    CD\n",
       " 19      months   NNS\n",
       " 20        from    IN\n",
       " 21         the    DT\n",
       " 22     passing    NN\n",
       " 23          of    IN\n",
       " 24         the    DT\n",
       " 25  resolution    NN\n",
       " 26           )     )\n",
       " 27          to    TO\n",
       " 28        file    VB\n",
       " 29         his  PRP$\n",
       " 30   complaint    NN\n",
       " 31           ,     ,\n",
       " 32         and    CC\n",
       " 33    actually    RB\n",
       " 34         did   VBD\n",
       " 35          so    RB\n",
       " 36          on    IN\n",
       " 37          29    CD\n",
       " 38    December   NNP\n",
       " 39        1998    CD\n",
       " 40           ,     ,\n",
       " 41          he   PRP\n",
       " 42         was   VBD\n",
       " 43        well    RB\n",
       " 44      within    IN\n",
       " 45         the    DT\n",
       " 46  limitation    NN\n",
       " 47      period    NN,\n",
       "             word  POS\n",
       " 0              1   CD\n",
       " 1              .    .\n",
       " 2              4   CD\n",
       " 3              .    .\n",
       " 4              1   CD\n",
       " 5              .    .\n",
       " 6              2   CD\n",
       " 7              .    .\n",
       " 8              3   CD\n",
       " 9              .    .\n",
       " 10            In   IN\n",
       " 11           the   DT\n",
       " 12          case   NN\n",
       " 13            of   IN\n",
       " 14             a   DT\n",
       " 15       vehicle   NN\n",
       " 16      equipped  VBN\n",
       " 17          with   IN\n",
       " 18            an   DT\n",
       " 19      electric   JJ\n",
       " 20  regenerative   NN\n",
       " 21       braking   NN\n",
       " 22        system   NN\n",
       " 23             ,    ,\n",
       " 24           the   DT\n",
       " 25  requirements  NNS\n",
       " 26        depend  VBP\n",
       " 27            on   IN\n",
       " 28           the   DT\n",
       " 29      category   NN\n",
       " 30            of   IN\n",
       " 31          this   DT\n",
       " 32        system   NN,\n",
       "               word   POS\n",
       " 0                A    DT\n",
       " 1            staff    NN\n",
       " 2           member    NN\n",
       " 3   misrepresented   VBN\n",
       " 4              and    CC\n",
       " 5        falsified   VBN\n",
       " 6              the    DT\n",
       " 7    certification    NN\n",
       " 8               of    IN\n",
       " 9        education    NN\n",
       " 10           grant    NN\n",
       " 11          claims   NNS\n",
       " 12       amounting   VBG\n",
       " 13              to    TO\n",
       " 14               $     $\n",
       " 15             129    CD\n",
       " 16               ,     ,\n",
       " 17             880    CD\n",
       " 18        relating    NN\n",
       " 19              to    TO\n",
       " 20             the    DT\n",
       " 21            1998    CD\n",
       " 22              to    TO\n",
       " 23            2002    CD\n",
       " 24        academic    JJ\n",
       " 25           years   NNS\n",
       " 26             for    IN\n",
       " 27             his  PRP$\n",
       " 28            four    CD\n",
       " 29        children   NNS,\n",
       "             word  POS\n",
       " 0            The   DT\n",
       " 1        authors  NNS\n",
       " 2              '  POS\n",
       " 3        appeals  NNS\n",
       " 4             to   TO\n",
       " 5            the   DT\n",
       " 6         Hessia  NNP\n",
       " 7            CDU  NNP\n",
       " 8          State  NNP\n",
       " 9          Party  NNP\n",
       " 10         Court  NNP\n",
       " 11           and   CC\n",
       " 12            to   TO\n",
       " 13           the   DT\n",
       " 14       Federal  NNP\n",
       " 15         Party  NNP\n",
       " 16         Court  NNP\n",
       " 17            at   IN\n",
       " 18          Bonn  NNP\n",
       " 19          were  VBD\n",
       " 20     dismissed  VBN\n",
       " 21            on   IN\n",
       " 22            26   CD\n",
       " 23       January  NNP\n",
       " 24          1996   CD\n",
       " 25           and   CC\n",
       " 26             ,    ,\n",
       " 27  respectively   RB\n",
       " 28             ,    ,\n",
       " 29            on   IN\n",
       " 30            24   CD\n",
       " 31     September  NNP\n",
       " 32          1996   CD,\n",
       "            word  POS\n",
       " 0             4   CD\n",
       " 1             .    .\n",
       " 2            At   IN\n",
       " 3      previous   JJ\n",
       " 4      sessions  NNS\n",
       " 5             ,    ,\n",
       " 6           the   DT\n",
       " 7         Fifth  NNP\n",
       " 8     Committee  NNP\n",
       " 9     submitted  VBD\n",
       " 10           to   TO\n",
       " 11          the   DT\n",
       " 12      General  NNP\n",
       " 13     Assembly  NNP\n",
       " 14            a   DT\n",
       " 15        draft   NN\n",
       " 16     decision   NN\n",
       " 17   containing  VBG\n",
       " 18          the   DT\n",
       " 19        names  NNS\n",
       " 20           of   IN\n",
       " 21          the   DT\n",
       " 22      persons  NNS\n",
       " 23  recommended  VBD\n",
       " 24          for   IN\n",
       " 25  appointment   NN,\n",
       "              word  POS\n",
       " 0               8   CD\n",
       " 1               .    .\n",
       " 2             The   DT\n",
       " 3         present   JJ\n",
       " 4          report   NN\n",
       " 5        provides  VBZ\n",
       " 6              an   DT\n",
       " 7        overview   NN\n",
       " 8              of   IN\n",
       " 9      activities  NNS\n",
       " 10     undertaken   JJ\n",
       " 11            and   CC\n",
       " 12   preparations  NNS\n",
       " 13          under   IN\n",
       " 14            way   NN\n",
       " 15             at   IN\n",
       " 16            the   DT\n",
       " 17       national   JJ\n",
       " 18              ,    ,\n",
       " 19       regional   JJ\n",
       " 20            and   CC\n",
       " 21  international   JJ\n",
       " 22         levels  NNS\n",
       " 23            for   IN\n",
       " 24            the   DT\n",
       " 25  international   JJ\n",
       " 26        meeting   NN\n",
       " 27             to   TO\n",
       " 28             be   VB\n",
       " 29           held  VBN\n",
       " 30             in   IN\n",
       " 31      Mauritius  NNP,\n",
       "           word  POS\n",
       " 0         Iraq  NNP\n",
       " 1       states  VBZ\n",
       " 2         that   IN\n",
       " 3   Ferrostaal  NNP\n",
       " 4      entered  VBD\n",
       " 5         into   IN\n",
       " 6            a   DT\n",
       " 7     contract   NN\n",
       " 8         with   IN\n",
       " 9         SCOP  NNP\n",
       " 10         for   IN\n",
       " 11         the   DT\n",
       " 12      supply   NN\n",
       " 13          of   IN\n",
       " 14        nine   CD\n",
       " 15     harbour   NN\n",
       " 16     vessels  NNS\n",
       " 17           ,    ,\n",
       " 18          of   IN\n",
       " 19       which  WDT\n",
       " 20        four   CD\n",
       " 21        were  VBD\n",
       " 22   delivered  VBN\n",
       " 23         and   CC\n",
       " 24         for   IN\n",
       " 25       which  WDT\n",
       " 26     payment   NN\n",
       " 27         was  VBD\n",
       " 28        made  VBN,\n",
       "             word  POS\n",
       " 0     Background  NNP\n",
       " 1    information   NN\n",
       " 2             on   IN\n",
       " 3            the   DT\n",
       " 4         report   NN\n",
       " ..           ...  ...\n",
       " 62           esa   NN\n",
       " 63             /  NNP\n",
       " 64  coordination   NN\n",
       " 65             /  NNP\n",
       " 66        ecosoc   NN\n",
       " \n",
       " [67 rows x 2 columns]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dic=list_dic_POStag1(sampled_lines[40000:40010])\n",
    "list_of_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd8f0db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Voice', 'reporting', 'via', 'VHF', '(', 'ch', '.', '5', '1', '2', '7', '))'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dic[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1658e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "pattern = '|'.join(stop_words)\n",
    "dg[0]['word']=dg[0]['word'].str.lower() \n",
    "dg[0]=dg[0][dg[0]['word'].str.contains(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07c40b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "12bbc679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happens</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happens</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twice</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  POS\n",
       "0     that   DT\n",
       "1  happens  VBZ\n",
       "2     that   IN\n",
       "3      the   DT\n",
       "4     word   NN\n",
       "6  happens  NNS\n",
       "7       is  VBZ\n",
       "8     said  VBD\n",
       "9    twice   RB"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30eac14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "def processed_sentences(list_of_df):\n",
    "    result=[]\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for df in list_of_df:\n",
    "        df['word']=df['word'].str.lower() \n",
    "        df=df.drop(df[df['word'].isin(stop_words)].index)\n",
    "        df=df.drop(df[~df['word'].str.isalpha()].index)\n",
    "        df.reset_index(drop=True, inplace=True)       \n",
    "        result.append(df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "362c825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           word  POS\n",
       " 0  proportional  NNP\n",
       " 1         means  NNS,\n",
       "         word  POS\n",
       " 0      voice  NNP\n",
       " 1  reporting  VBG\n",
       " 2        via   IN\n",
       " 3        vhf  NNP\n",
       " 4         ch   NN,\n",
       "           word  POS\n",
       " 0    therefore   RB\n",
       " 1       author   NN\n",
       " 2       argues  VBZ\n",
       " 3        least  JJS\n",
       " 4      january  NNP\n",
       " 5          two   CD\n",
       " 6       months  NNS\n",
       " 7      passing   NN\n",
       " 8   resolution   NN\n",
       " 9         file   VB\n",
       " 10   complaint   NN\n",
       " 11    actually   RB\n",
       " 12    december  NNP\n",
       " 13        well   RB\n",
       " 14      within   IN\n",
       " 15  limitation   NN\n",
       " 16      period   NN,\n",
       "             word  POS\n",
       " 0           case   NN\n",
       " 1        vehicle   NN\n",
       " 2       equipped  VBN\n",
       " 3       electric   JJ\n",
       " 4   regenerative   NN\n",
       " 5        braking   NN\n",
       " 6         system   NN\n",
       " 7   requirements  NNS\n",
       " 8         depend  VBP\n",
       " 9       category   NN\n",
       " 10        system   NN,\n",
       "               word  POS\n",
       " 0            staff   NN\n",
       " 1           member   NN\n",
       " 2   misrepresented  VBN\n",
       " 3        falsified  VBN\n",
       " 4    certification   NN\n",
       " 5        education   NN\n",
       " 6            grant   NN\n",
       " 7           claims  NNS\n",
       " 8        amounting  VBG\n",
       " 9         relating   NN\n",
       " 10        academic   JJ\n",
       " 11           years  NNS\n",
       " 12            four   CD\n",
       " 13        children  NNS,\n",
       "             word  POS\n",
       " 0        authors  NNS\n",
       " 1        appeals  NNS\n",
       " 2         hessia  NNP\n",
       " 3            cdu  NNP\n",
       " 4          state  NNP\n",
       " 5          party  NNP\n",
       " 6          court  NNP\n",
       " 7        federal  NNP\n",
       " 8          party  NNP\n",
       " 9          court  NNP\n",
       " 10          bonn  NNP\n",
       " 11     dismissed  VBN\n",
       " 12       january  NNP\n",
       " 13  respectively   RB\n",
       " 14     september  NNP,\n",
       "            word  POS\n",
       " 0      previous   JJ\n",
       " 1      sessions  NNS\n",
       " 2         fifth  NNP\n",
       " 3     committee  NNP\n",
       " 4     submitted  VBD\n",
       " 5       general  NNP\n",
       " 6      assembly  NNP\n",
       " 7         draft   NN\n",
       " 8      decision   NN\n",
       " 9    containing  VBG\n",
       " 10        names  NNS\n",
       " 11      persons  NNS\n",
       " 12  recommended  VBD\n",
       " 13  appointment   NN,\n",
       "              word  POS\n",
       " 0         present   JJ\n",
       " 1          report   NN\n",
       " 2        provides  VBZ\n",
       " 3        overview   NN\n",
       " 4      activities  NNS\n",
       " 5      undertaken   JJ\n",
       " 6    preparations  NNS\n",
       " 7             way   NN\n",
       " 8        national   JJ\n",
       " 9        regional   JJ\n",
       " 10  international   JJ\n",
       " 11         levels  NNS\n",
       " 12  international   JJ\n",
       " 13        meeting   NN\n",
       " 14           held  VBN\n",
       " 15      mauritius  NNP,\n",
       "           word  POS\n",
       " 0         iraq  NNP\n",
       " 1       states  VBZ\n",
       " 2   ferrostaal  NNP\n",
       " 3      entered  VBD\n",
       " 4     contract   NN\n",
       " 5         scop  NNP\n",
       " 6       supply   NN\n",
       " 7         nine   CD\n",
       " 8      harbour   NN\n",
       " 9      vessels  NNS\n",
       " 10        four   CD\n",
       " 11   delivered  VBN\n",
       " 12     payment   NN\n",
       " 13        made  VBN,\n",
       "             word  POS\n",
       " 0     background  NNP\n",
       " 1    information   NN\n",
       " 2         report   NN\n",
       " 3      secretary  NNP\n",
       " 4        general  NNP\n",
       " 5           high   JJ\n",
       " 6          level   NN\n",
       " 7        segment   NN\n",
       " 8       economic  NNP\n",
       " 9         social  NNP\n",
       " 10       council  NNP\n",
       " 11         theme   NN\n",
       " 12     promoting  VBG\n",
       " 13    integrated  VBN\n",
       " 14      approach   NN\n",
       " 15         rural   JJ\n",
       " 16   development   NN\n",
       " 17    developing  VBG\n",
       " 18     countries  NNS\n",
       " 19       poverty   NN\n",
       " 20   eradication   NN\n",
       " 21   sustainable   JJ\n",
       " 22   development   NN\n",
       " 23     available   JJ\n",
       " 24           web  NNP\n",
       " 25          site   NN\n",
       " 26      economic  NNP\n",
       " 27        social  NNP\n",
       " 28       council  NNP\n",
       " 29           www   NN\n",
       " 30            un   JJ\n",
       " 31           org   JJ\n",
       " 32           esa   NN\n",
       " 33  coordination   NN\n",
       " 34        ecosoc   NN]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences(list_of_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "80f3dc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           word  POS\n",
       " 0             d   NN\n",
       " 1             )    )\n",
       " 2  proportional  NNP\n",
       " 3         means  NNS,\n",
       "          word  POS\n",
       " 0       voice  NNP\n",
       " 1   reporting  VBG\n",
       " 2         via   IN\n",
       " 3         vhf  NNP\n",
       " 4           (    (\n",
       " 5          ch   NN\n",
       " 6           .    .\n",
       " 7           5   CD\n",
       " 8           .    .\n",
       " 9           1   CD\n",
       " 10          .    .\n",
       " 11          2   CD\n",
       " 12          (    (\n",
       " 13          7   CD\n",
       " 14         ))   NN,\n",
       "           word   POS\n",
       " 0    therefore    RB\n",
       " 1            ,     ,\n",
       " 2          the    DT\n",
       " 3       author    NN\n",
       " 4       argues   VBZ\n",
       " 5         that    RB\n",
       " 6            ,     ,\n",
       " 7           as    IN\n",
       " 8           he   PRP\n",
       " 9          had   VBD\n",
       " 10          at    IN\n",
       " 11       least   JJS\n",
       " 12          up    RB\n",
       " 13       until    IN\n",
       " 14           5    CD\n",
       " 15     january   NNP\n",
       " 16        1999    CD\n",
       " 17           (     (\n",
       " 18         two    CD\n",
       " 19      months   NNS\n",
       " 20        from    IN\n",
       " 21         the    DT\n",
       " 22     passing    NN\n",
       " 23          of    IN\n",
       " 24         the    DT\n",
       " 25  resolution    NN\n",
       " 26           )     )\n",
       " 27          to    TO\n",
       " 28        file    VB\n",
       " 29         his  PRP$\n",
       " 30   complaint    NN\n",
       " 31           ,     ,\n",
       " 32         and    CC\n",
       " 33    actually    RB\n",
       " 34         did   VBD\n",
       " 35          so    RB\n",
       " 36          on    IN\n",
       " 37          29    CD\n",
       " 38    december   NNP\n",
       " 39        1998    CD\n",
       " 40           ,     ,\n",
       " 41          he   PRP\n",
       " 42         was   VBD\n",
       " 43        well    RB\n",
       " 44      within    IN\n",
       " 45         the    DT\n",
       " 46  limitation    NN\n",
       " 47      period    NN,\n",
       "             word  POS\n",
       " 0              1   CD\n",
       " 1              .    .\n",
       " 2              4   CD\n",
       " 3              .    .\n",
       " 4              1   CD\n",
       " 5              .    .\n",
       " 6              2   CD\n",
       " 7              .    .\n",
       " 8              3   CD\n",
       " 9              .    .\n",
       " 10            in   IN\n",
       " 11           the   DT\n",
       " 12          case   NN\n",
       " 13            of   IN\n",
       " 14             a   DT\n",
       " 15       vehicle   NN\n",
       " 16      equipped  VBN\n",
       " 17          with   IN\n",
       " 18            an   DT\n",
       " 19      electric   JJ\n",
       " 20  regenerative   NN\n",
       " 21       braking   NN\n",
       " 22        system   NN\n",
       " 23             ,    ,\n",
       " 24           the   DT\n",
       " 25  requirements  NNS\n",
       " 26        depend  VBP\n",
       " 27            on   IN\n",
       " 28           the   DT\n",
       " 29      category   NN\n",
       " 30            of   IN\n",
       " 31          this   DT\n",
       " 32        system   NN,\n",
       "               word   POS\n",
       " 0                a    DT\n",
       " 1            staff    NN\n",
       " 2           member    NN\n",
       " 3   misrepresented   VBN\n",
       " 4              and    CC\n",
       " 5        falsified   VBN\n",
       " 6              the    DT\n",
       " 7    certification    NN\n",
       " 8               of    IN\n",
       " 9        education    NN\n",
       " 10           grant    NN\n",
       " 11          claims   NNS\n",
       " 12       amounting   VBG\n",
       " 13              to    TO\n",
       " 14               $     $\n",
       " 15             129    CD\n",
       " 16               ,     ,\n",
       " 17             880    CD\n",
       " 18        relating    NN\n",
       " 19              to    TO\n",
       " 20             the    DT\n",
       " 21            1998    CD\n",
       " 22              to    TO\n",
       " 23            2002    CD\n",
       " 24        academic    JJ\n",
       " 25           years   NNS\n",
       " 26             for    IN\n",
       " 27             his  PRP$\n",
       " 28            four    CD\n",
       " 29        children   NNS,\n",
       "             word  POS\n",
       " 0            the   DT\n",
       " 1        authors  NNS\n",
       " 2              '  POS\n",
       " 3        appeals  NNS\n",
       " 4             to   TO\n",
       " 5            the   DT\n",
       " 6         hessia  NNP\n",
       " 7            cdu  NNP\n",
       " 8          state  NNP\n",
       " 9          party  NNP\n",
       " 10         court  NNP\n",
       " 11           and   CC\n",
       " 12            to   TO\n",
       " 13           the   DT\n",
       " 14       federal  NNP\n",
       " 15         party  NNP\n",
       " 16         court  NNP\n",
       " 17            at   IN\n",
       " 18          bonn  NNP\n",
       " 19          were  VBD\n",
       " 20     dismissed  VBN\n",
       " 21            on   IN\n",
       " 22            26   CD\n",
       " 23       january  NNP\n",
       " 24          1996   CD\n",
       " 25           and   CC\n",
       " 26             ,    ,\n",
       " 27  respectively   RB\n",
       " 28             ,    ,\n",
       " 29            on   IN\n",
       " 30            24   CD\n",
       " 31     september  NNP\n",
       " 32          1996   CD,\n",
       "            word  POS\n",
       " 0             4   CD\n",
       " 1             .    .\n",
       " 2            at   IN\n",
       " 3      previous   JJ\n",
       " 4      sessions  NNS\n",
       " 5             ,    ,\n",
       " 6           the   DT\n",
       " 7         fifth  NNP\n",
       " 8     committee  NNP\n",
       " 9     submitted  VBD\n",
       " 10           to   TO\n",
       " 11          the   DT\n",
       " 12      general  NNP\n",
       " 13     assembly  NNP\n",
       " 14            a   DT\n",
       " 15        draft   NN\n",
       " 16     decision   NN\n",
       " 17   containing  VBG\n",
       " 18          the   DT\n",
       " 19        names  NNS\n",
       " 20           of   IN\n",
       " 21          the   DT\n",
       " 22      persons  NNS\n",
       " 23  recommended  VBD\n",
       " 24          for   IN\n",
       " 25  appointment   NN,\n",
       "              word  POS\n",
       " 0               8   CD\n",
       " 1               .    .\n",
       " 2             the   DT\n",
       " 3         present   JJ\n",
       " 4          report   NN\n",
       " 5        provides  VBZ\n",
       " 6              an   DT\n",
       " 7        overview   NN\n",
       " 8              of   IN\n",
       " 9      activities  NNS\n",
       " 10     undertaken   JJ\n",
       " 11            and   CC\n",
       " 12   preparations  NNS\n",
       " 13          under   IN\n",
       " 14            way   NN\n",
       " 15             at   IN\n",
       " 16            the   DT\n",
       " 17       national   JJ\n",
       " 18              ,    ,\n",
       " 19       regional   JJ\n",
       " 20            and   CC\n",
       " 21  international   JJ\n",
       " 22         levels  NNS\n",
       " 23            for   IN\n",
       " 24            the   DT\n",
       " 25  international   JJ\n",
       " 26        meeting   NN\n",
       " 27             to   TO\n",
       " 28             be   VB\n",
       " 29           held  VBN\n",
       " 30             in   IN\n",
       " 31      mauritius  NNP,\n",
       "           word  POS\n",
       " 0         iraq  NNP\n",
       " 1       states  VBZ\n",
       " 2         that   IN\n",
       " 3   ferrostaal  NNP\n",
       " 4      entered  VBD\n",
       " 5         into   IN\n",
       " 6            a   DT\n",
       " 7     contract   NN\n",
       " 8         with   IN\n",
       " 9         scop  NNP\n",
       " 10         for   IN\n",
       " 11         the   DT\n",
       " 12      supply   NN\n",
       " 13          of   IN\n",
       " 14        nine   CD\n",
       " 15     harbour   NN\n",
       " 16     vessels  NNS\n",
       " 17           ,    ,\n",
       " 18          of   IN\n",
       " 19       which  WDT\n",
       " 20        four   CD\n",
       " 21        were  VBD\n",
       " 22   delivered  VBN\n",
       " 23         and   CC\n",
       " 24         for   IN\n",
       " 25       which  WDT\n",
       " 26     payment   NN\n",
       " 27         was  VBD\n",
       " 28        made  VBN,\n",
       "             word  POS\n",
       " 0     background  NNP\n",
       " 1    information   NN\n",
       " 2             on   IN\n",
       " 3            the   DT\n",
       " 4         report   NN\n",
       " ..           ...  ...\n",
       " 62           esa   NN\n",
       " 63             /  NNP\n",
       " 64  coordination   NN\n",
       " 65             /  NNP\n",
       " 66        ecosoc   NN\n",
       " \n",
       " [67 rows x 2 columns]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ee2a3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192b9d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        word  POS\n",
       " 0     thirty  NNP\n",
       " 1     fourth   JJ\n",
       " 2    session   NN\n",
       " 3     geneva  NNP\n",
       " 4  september  NNP,\n",
       "     word  POS\n",
       " 0     mr  NNP\n",
       " 1  walid  NNP\n",
       " 2     al  NNP\n",
       " 3  hadid   NN,\n",
       "            word  POS\n",
       " 0          also   RB\n",
       " 1    considered  VBN\n",
       " 2  unacceptable   JJ\n",
       " 3        public   JJ\n",
       " 4         funds  NNS\n",
       " 5          used  VBN\n",
       " 6    compensate   VB\n",
       " 7          loss   NN\n",
       " 8     allocated  VBN\n",
       " 9      operator   NN,\n",
       "         word  POS\n",
       " 0     inland  NNP\n",
       " 1  transport  NNP\n",
       " 2  committee  NNP,\n",
       "       word  POS\n",
       " 0       mr  NNP\n",
       " 1        j  NNP\n",
       " 2        f  NNP\n",
       " 3  kissack   VB,\n",
       "           word  POS\n",
       " 0       issues  NNS\n",
       " 1       needed  VBN\n",
       " 2     response   NN\n",
       " 3  highlighted  VBN\n",
       " 4       almaty  NNP\n",
       " 5    programme  NNP\n",
       " 6       action  NNP,\n",
       "              word   POS\n",
       " 0         dealing   VBG\n",
       " 1          agenda    NN\n",
       " 2            item    NN\n",
       " 3        entitled   VBD\n",
       " 4          oceans  NNPS\n",
       " 5             law    NN\n",
       " 6             sea    NN\n",
       " 7             day    NN\n",
       " 8     celebrating   VBG\n",
       " 9           tenth    JJ\n",
       " 10    anniversary    NN\n",
       " 11          entry    NN\n",
       " 12          force    NN\n",
       " 13         united   NNP\n",
       " 14        nations  NNPS\n",
       " 15     convention   NNP\n",
       " 16            law   NNP\n",
       " 17            sea   NNP\n",
       " 18           text    NN\n",
       " 19       historic    JJ\n",
       " 20         impact    NN\n",
       " 21     convention    NN\n",
       " 22           made   VBN\n",
       " 23   indisputable    JJ\n",
       " 24   contribution    NN\n",
       " 25   codification    NN\n",
       " 26  international    JJ\n",
       " 27            law    NN\n",
       " 28            sea    NN\n",
       " 29      important    JJ\n",
       " 30      milestone    NN\n",
       " 31  establishment    NN\n",
       " 32         global    JJ\n",
       " 33          legal    JJ\n",
       " 34      framework    NN\n",
       " 35     governance    NN\n",
       " 36        various    JJ\n",
       " 37         marine    JJ\n",
       " 38          areas   NNS\n",
       " 39         living    NN\n",
       " 40            non    JJ\n",
       " 41         living    NN\n",
       " 42      resources   NNS,\n",
       "           word  POS\n",
       " 0       latter   NN\n",
       " 1        would   MD\n",
       " 2     continue   VB\n",
       " 3     reviewed  VBN\n",
       " 4      unmovic  NNP\n",
       " 5         iaea  NNP\n",
       " 6       ensure   VB\n",
       " 7        items  NNS\n",
       " 8     included  VBN\n",
       " 9        goods  NNS\n",
       " 10      review  VBP\n",
       " 11        list   NN\n",
       " 12   otherwise   RB\n",
       " 13     subject   JJ\n",
       " 14   paragraph   VB\n",
       " 15  resolution   NN,\n",
       "           word  POS\n",
       " 0   government  NNP\n",
       " 1       placed  VBN\n",
       " 2       issues  NNS\n",
       " 3         high  VBP\n",
       " 4       agenda   NN\n",
       " 5       sought  VBN\n",
       " 6          new   JJ\n",
       " 7     measures  NNS\n",
       " 8     tailored  VBN\n",
       " 9        needs  NNS\n",
       " 10      modern   JJ\n",
       " 11     society   NN,\n",
       "          word  POS\n",
       " 0  guatemalan  NNP\n",
       " 1      social  NNP\n",
       " 2    security  NNP\n",
       " 3   institute  NNP\n",
       " 4        igss  NNP\n",
       " 5         per   IN\n",
       " 6        cent   NN]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences = mc.process_sentences(sampled_lines)\n",
    "processed_sentences[40000:40010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caffda8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[40009].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d61ac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6368c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first drop the sentences with less than 5 words:\n",
    "long_enough_sentences=[df for df in processed_sentences if df.shape[0]>=5]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8c5292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_enough_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196841b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we pick randomly 50000 of the left sentences\n",
    "samples=50000\n",
    "import random\n",
    "if samples>len(long_enough_sentences):\n",
    "    raise Exception(\"pick a smaller number of samples or a bigger list of processed_sentences\")\n",
    "all_sentences=random.sample(long_enough_sentences, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d348ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c696d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each df in all_sentences, pick a random 5 elements window\n",
    "all_samp=[]\n",
    "for df in all_sentences:\n",
    "    n=random.randint(0,df.shape[0]-5)\n",
    "    dg=df.iloc[n:n+5,:]\n",
    "    all_samp.append(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f231f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_samples(processed_sentences, samples):\n",
    "    #first drop the sentences with less than 5 words:\n",
    "    long_enough_sentences=[df for df in processed_sentences if df.shape[0]>=5]\n",
    "    #then we pick randomly 50000 of the left sentences\n",
    "    if samples>len(long_enough_sentences):\n",
    "        raise Exception(\"pick a smaller number of samples or a bigger list of processed_sentences\")\n",
    "    all_sentences=random.sample(long_enough_sentences, samples)\n",
    "    #for each df in all_sentences, pick a random 5 elements window\n",
    "    all_samp=[]\n",
    "    for df in all_sentences:\n",
    "        n=random.randint(0,df.shape[0]-5)\n",
    "        dg=df.iloc[n:n+5,:]\n",
    "        all_samp.append(dg)\n",
    "    \n",
    "    return all_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0d376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samp=create_samples(processed_sentences,samples=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1563f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples=all_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5e3312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670bd928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[              word  POS\n",
      "12        republic  NNP\n",
      "13       lithuania  NNP\n",
      "14        prohibit   VB\n",
      "15  discrimination   NN\n",
      "16          direct   JJ,       word POS\n",
      "4  project  NN\n",
      "5  manager  NN\n",
      "6    would  MD\n",
      "7  oversee  VB\n",
      "8    pilot  NN,         word  POS\n",
      "0      price   NN\n",
      "1   quantity   NN\n",
      "2    indices  NNS\n",
      "3  transport   NN\n",
      "4   services  NNS,              word   POS\n",
      "15  international    JJ\n",
      "16          organ    NN\n",
      "17         united   NNP\n",
      "18        nations  NNPS\n",
      "19         unique    JJ,             word  POS\n",
      "3      indicates  VBZ\n",
      "4  participation   NN\n",
      "5        parents  NNS\n",
      "6       children  NNS\n",
      "7         always   RB,           word  POS\n",
      "3  testimonies  NNS\n",
      "4      abusive   JJ\n",
      "5    treatment   NN\n",
      "6        girls  NNS\n",
      "7        women  NNS,              word  POS\n",
      "3            anti  NNP\n",
      "4  discrimination   NN\n",
      "5            unit  NNP\n",
      "6  implementation   NN\n",
      "7      commission  NNP,             word  POS\n",
      "11        forces  NNS\n",
      "12  investigated  VBN\n",
      "13         order   NN\n",
      "14           put   VB\n",
      "15           end   NN,           word  POS\n",
      "11  illustrate   VB\n",
      "12  phenomenon   NN\n",
      "13       today   NN\n",
      "14  pioneering  VBG\n",
      "15     efforts  NNS,       word  POS\n",
      "5     line   NN\n",
      "6  militia   NN\n",
      "7    still   RB\n",
      "8  remains  VBZ\n",
      "9     west  NNP]\n"
     ]
    }
   ],
   "source": [
    "all_samples = mc.create_samples(processed_sentences, samples=50000)\n",
    "\n",
    "print(all_samples[25000:25010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6be6d",
   "metadata": {},
   "source": [
    "## Part 3 - convert the samples into a Pandas DataFrame (10 points)\n",
    "\n",
    "Here, you will take the samples and create a table whose columns are the features and the class and whose rows are the samples.  All the features and the class will be binary.  Note that there may be many columns, in the hundreds or thousands depending on the diversity of the final two consonants of the non-stop-words in the dataset, but the sum of all rows will be five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ef7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#First we transform each df in the following array: first element= last two letters of the first word\n",
    "                                                    #second elt = last two letters of the second word\n",
    "                                                    #third elt= last two letters of the fourth word\n",
    "                                                    #fourth elt = last two letters of the 5th word\n",
    "                                                    #fifth element = POS of the third word\n",
    "def transform(all_samples):\n",
    "    trans=[]\n",
    "    for df in all_samples:\n",
    "        arr=np.zeros(5,dtype=object)\n",
    "        arr[0]=df['word'].iloc[0][-2:]\n",
    "        arr[1]=df['word'].iloc[1][-2:]\n",
    "        arr[2]=df['word'].iloc[3][-2:]\n",
    "        arr[3]=df['word'].iloc[4][-2:]\n",
    "        arr[4]=df['POS'].iloc[2]\n",
    "        trans.append(arr)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5358ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=transform(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204e5ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['rs', 'ft', 'ed', 'nt', 'NN'], dtype=object),\n",
       " array(['ed', 'ry', 'ms', 'ct', 'JJ'], dtype=object),\n",
       " array(['et', 'te', 'ps', 'as', 'JJ'], dtype=object),\n",
       " array(['io', 'ts', 'al', 'ed', 'RB'], dtype=object),\n",
       " array(['ny', 'ng', 'ed', 'ed', 'NN'], dtype=object),\n",
       " array(['ee', 'nt', 'ns', 'ry', 'NNP'], dtype=object),\n",
       " array(['ed', 'ly', 'ss', 'on', 'VBG'], dtype=object),\n",
       " array(['rs', 'ct', 'ct', 'pc', 'VBD'], dtype=object),\n",
       " array(['sm', 'nt', 'ws', 'ns', 'NNP'], dtype=object),\n",
       " array(['nd', 'so', 'id', 'ns', 'VBZ'], dtype=object)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71899a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We join them in one matrix:\n",
    "matrice=np.stack(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "800701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We chose a class to classify: VBZ\n",
    "matrice[:,4]=1*(matrice[:,4]=='NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "777f7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg=pd.DataFrame(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40a35507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>6659</td>\n",
       "      <td>6659</td>\n",
       "      <td>6659</td>\n",
       "      <td>6659</td>\n",
       "      <td>6659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJR</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJS</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>14152</td>\n",
       "      <td>14152</td>\n",
       "      <td>14152</td>\n",
       "      <td>14152</td>\n",
       "      <td>14152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>9940</td>\n",
       "      <td>9940</td>\n",
       "      <td>9940</td>\n",
       "      <td>9940</td>\n",
       "      <td>9940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>6803</td>\n",
       "      <td>6803</td>\n",
       "      <td>6803</td>\n",
       "      <td>6803</td>\n",
       "      <td>6803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDT</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBR</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>2344</td>\n",
       "      <td>2344</td>\n",
       "      <td>2344</td>\n",
       "      <td>2344</td>\n",
       "      <td>2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>1682</td>\n",
       "      <td>1682</td>\n",
       "      <td>1682</td>\n",
       "      <td>1682</td>\n",
       "      <td>1682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>2984</td>\n",
       "      <td>2984</td>\n",
       "      <td>2984</td>\n",
       "      <td>2984</td>\n",
       "      <td>2984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBP</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDT</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP$</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRB</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4\n",
       "4                                      \n",
       "$         2      2      2      2      2\n",
       "CC       11     11     11     11     11\n",
       "CD      326    326    326    326    326\n",
       "DT       46     46     46     46     46\n",
       "EX        1      1      1      1      1\n",
       "FW       37     37     37     37     37\n",
       "IN      490    490    490    490    490\n",
       "JJ     6659   6659   6659   6659   6659\n",
       "JJR     104    104    104    104    104\n",
       "JJS      84     84     84     84     84\n",
       "MD      583    583    583    583    583\n",
       "NN    14152  14152  14152  14152  14152\n",
       "NNP    9940   9940   9940   9940   9940\n",
       "NNPS    413    413    413    413    413\n",
       "NNS    6803   6803   6803   6803   6803\n",
       "PDT       6      6      6      6      6\n",
       "PRP      19     19     19     19     19\n",
       "RB     1308   1308   1308   1308   1308\n",
       "RBR      15     15     15     15     15\n",
       "RBS       1      1      1      1      1\n",
       "RP        6      6      6      6      6\n",
       "SYM      38     38     38     38     38\n",
       "VB     2344   2344   2344   2344   2344\n",
       "VBD    1050   1050   1050   1050   1050\n",
       "VBG    1682   1682   1682   1682   1682\n",
       "VBN    2984   2984   2984   2984   2984\n",
       "VBP     338    338    338    338    338\n",
       "VBZ     540    540    540    540    540\n",
       "WDT       1      1      1      1      1\n",
       "WP$      13     13     13     13     13\n",
       "WRB       4      4      4      4      4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.groupby(dg.iloc[:,4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9e229c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "m=enc.fit_transform(matrice[:,:4]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee2092ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_final=np.concatenate((m,matrice[:,4][:,None]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3223f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 16.0, 23.0, ..., 1.0, 1.0, 14152], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_final.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09d2dac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'ak', 'al',\n",
       "        'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw',\n",
       "        'ax', 'ay', 'az', 'b', 'ba', 'bb', 'bc', 'bd', 'be', 'bh', 'bi',\n",
       "        'bj', 'bm', 'bo', 'bq', 'bs', 'bt', 'bv', 'bw', 'by', 'c', 'ca',\n",
       "        'cc', 'cd', 'ce', 'cf', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn',\n",
       "        'co', 'cp', 'cr', 'cs', 'ct', 'cw', 'cx', 'cy', 'da', 'db', 'dc',\n",
       "        'dd', 'de', 'df', 'dg', 'di', 'dj', 'dm', 'dn', 'do', 'dp', 'dr',\n",
       "        'ds', 'dt', 'du', 'dy', 'e', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef',\n",
       "        'eg', 'ei', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'er', 'es', 'et',\n",
       "        'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'f', 'fc', 'fe', 'ff', 'fn',\n",
       "        'fp', 'fs', 'ft', 'fu', 'fy', 'g', 'ga', 'gb', 'gc', 'ge', 'gg',\n",
       "        'gh', 'gi', 'gm', 'gn', 'go', 'gs', 'gu', 'gy', 'h', 'ha', 'hc',\n",
       "        'hd', 'he', 'hg', 'hi', 'hl', 'hn', 'ho', 'hr', 'hs', 'ht', 'hw',\n",
       "        'hy', 'ia', 'ib', 'ic', 'id', 'ie', 'ig', 'ii', 'ik', 'il', 'im',\n",
       "        'in', 'io', 'ip', 'ir', 'is', 'it', 'iv', 'ix', 'j', 'ja', 'ji',\n",
       "        'jo', 'jy', 'k', 'ka', 'ke', 'kh', 'ki', 'km', 'ko', 'kr', 'ks',\n",
       "        'ku', 'l', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'li', 'lk', 'll',\n",
       "        'lm', 'ln', 'lo', 'lp', 'ls', 'lt', 'lu', 'ly', 'ma', 'mb', 'mc',\n",
       "        'md', 'me', 'mf', 'mg', 'mi', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp',\n",
       "        'mr', 'ms', 'mu', 'mw', 'my', 'n', 'na', 'nc', 'nd', 'ne', 'nf',\n",
       "        'ng', 'ni', 'nk', 'nl', 'nm', 'nn', 'no', 'ns', 'nt', 'nu', 'ny',\n",
       "        'nz', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'ok',\n",
       "        'ol', 'om', 'on', 'oo', 'op', 'or', 'os', 'ot', 'ou', 'ov', 'ow',\n",
       "        'ox', 'oy', 'p', 'pa', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi',\n",
       "        'pl', 'pm', 'po', 'pp', 'pr', 'ps', 'pt', 'pu', 'pv', 'py', 'q',\n",
       "        'qa', 'qi', 'qu', 'r', 'ra', 'rc', 'rd', 're', 'rf', 'rg', 'rh',\n",
       "        'ri', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rr', 'rs', 'rt', 'ru',\n",
       "        'rw', 'ry', 'sa', 'sc', 'sd', 'se', 'sg', 'sh', 'si', 'sk', 'sm',\n",
       "        'so', 'sp', 'ss', 'st', 'su', 'sy', 'ta', 'tb', 'tc', 'td', 'te',\n",
       "        'tf', 'th', 'ti', 'tl', 'to', 'tp', 'tr', 'ts', 'tu', 'tv', 'tx',\n",
       "        'ty', 'tz', 'u', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uk',\n",
       "        'ul', 'um', 'un', 'up', 'ur', 'us', 'ut', 'uu', 'uv', 'ux', 'uy',\n",
       "        'uz', 'v', 'va', 've', 'vi', 'vl', 'vn', 'vo', 'vu', 'vy', 'w',\n",
       "        'wa', 'wc', 'we', 'wf', 'wg', 'wi', 'wn', 'wo', 'wp', 'ws', 'ww',\n",
       "        'x', 'xb', 'xc', 'xi', 'xt', 'xv', 'xx', 'xy', 'ya', 'yd', 'ye',\n",
       "        'yi', 'yl', 'yo', 'yp', 'ys', 'yx', 'yz', 'za', 'ze', 'zk', 'zo',\n",
       "        'zs'], dtype=object),\n",
       " array(['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'ak', 'al',\n",
       "        'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw',\n",
       "        'ax', 'ay', 'az', 'b', 'ba', 'bb', 'bc', 'be', 'bi', 'bo', 'bq',\n",
       "        'br', 'bs', 'bt', 'bu', 'by', 'c', 'ca', 'cb', 'cc', 'cd', 'ce',\n",
       "        'cf', 'ch', 'ci', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cr', 'cs',\n",
       "        'ct', 'cu', 'cy', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh',\n",
       "        'di', 'dn', 'do', 'dp', 'dr', 'ds', 'dt', 'du', 'dx', 'dy', 'e',\n",
       "        'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ek', 'el',\n",
       "        'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew',\n",
       "        'ex', 'ey', 'ez', 'f', 'fa', 'fc', 'fe', 'ff', 'fi', 'fl', 'fp',\n",
       "        'fs', 'ft', 'fy', 'g', 'ga', 'gc', 'gd', 'ge', 'gf', 'gh', 'gi',\n",
       "        'gm', 'gn', 'go', 'gp', 'gs', 'gu', 'gy', 'h', 'ha', 'hc', 'he',\n",
       "        'hf', 'hi', 'hl', 'ho', 'hq', 'hr', 'hs', 'ht', 'hu', 'hy', 'ia',\n",
       "        'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ii', 'ik', 'il', 'im', 'in',\n",
       "        'io', 'ip', 'ir', 'is', 'it', 'iu', 'iv', 'ix', 'j', 'ja', 'ji',\n",
       "        'k', 'ka', 'ke', 'kg', 'kh', 'ki', 'kj', 'kk', 'km', 'ko', 'ks',\n",
       "        'l', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'li', 'lk', 'll', 'lm',\n",
       "        'ln', 'lo', 'lp', 'lr', 'ls', 'lt', 'ly', 'ma', 'mb', 'me', 'mf',\n",
       "        'mg', 'mi', 'mm', 'mn', 'mo', 'mp', 'mr', 'ms', 'mt', 'mu', 'mw',\n",
       "        'my', 'n', 'na', 'nc', 'nd', 'ne', 'nf', 'ng', 'ni', 'nj', 'nk',\n",
       "        'nm', 'nn', 'no', 'ns', 'nt', 'nu', 'nv', 'ny', 'nz', 'oa', 'ob',\n",
       "        'oc', 'od', 'of', 'og', 'oh', 'ok', 'ol', 'om', 'on', 'op', 'or',\n",
       "        'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'p', 'pa', 'pc', 'pd',\n",
       "        'pe', 'pf', 'pg', 'ph', 'pi', 'pl', 'pm', 'pp', 'pr', 'ps', 'pt',\n",
       "        'pw', 'py', 'q', 'qi', 'qu', 'r', 'ra', 'rb', 'rc', 'rd', 're',\n",
       "        'rf', 'rg', 'rh', 'ri', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rr',\n",
       "        'rs', 'rt', 'ru', 'rw', 'ry', 'sa', 'sb', 'sc', 'sd', 'se', 'sg',\n",
       "        'sh', 'si', 'sk', 'sm', 'so', 'sp', 'sr', 'ss', 'st', 'su', 'sy',\n",
       "        'ta', 'tb', 'tc', 'td', 'te', 'tf', 'th', 'ti', 'tk', 'to', 'tp',\n",
       "        'tr', 'ts', 'tt', 'tu', 'tv', 'ty', 'tz', 'ua', 'ub', 'uc', 'ud',\n",
       "        'ue', 'uf', 'ug', 'ul', 'um', 'un', 'up', 'ur', 'us', 'ut', 'v',\n",
       "        'va', 've', 'vi', 'vl', 'vo', 'vp', 'vy', 'w', 'wa', 'wc', 'we',\n",
       "        'wg', 'wi', 'wn', 'wo', 'wp', 'ws', 'ww', 'x', 'xa', 'xb', 'xi',\n",
       "        'xt', 'xv', 'ya', 'yd', 'yi', 'yl', 'yp', 'ys', 'yt', 'yz', 'za',\n",
       "        'ze', 'zi', 'zo'], dtype=object),\n",
       " array(['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'ak', 'al',\n",
       "        'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw',\n",
       "        'ax', 'ay', 'az', 'b', 'ba', 'bc', 'bd', 'be', 'bi', 'bm', 'bo',\n",
       "        'bp', 'bq', 'br', 'bs', 'bt', 'by', 'c', 'ca', 'cb', 'cc', 'cd',\n",
       "        'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cm', 'cn', 'co', 'cp',\n",
       "        'cr', 'cs', 'ct', 'cu', 'cw', 'cy', 'cz', 'da', 'db', 'dc', 'dd',\n",
       "        'de', 'df', 'dg', 'di', 'dm', 'dn', 'do', 'dp', 'dr', 'ds', 'dt',\n",
       "        'du', 'dy', 'e', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh',\n",
       "        'ei', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'er', 'es', 'et', 'eu',\n",
       "        'ev', 'ew', 'ex', 'ey', 'ez', 'f', 'fa', 'fc', 'fe', 'ff', 'fi',\n",
       "        'fm', 'fp', 'fr', 'fs', 'ft', 'fy', 'g', 'ga', 'gb', 'gc', 'ge',\n",
       "        'gg', 'gh', 'gm', 'gn', 'go', 'gs', 'gu', 'gy', 'h', 'ha', 'hd',\n",
       "        'he', 'hf', 'hg', 'hi', 'hl', 'hn', 'ho', 'hr', 'hs', 'ht', 'hw',\n",
       "        'hy', 'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii',\n",
       "        'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu',\n",
       "        'iv', 'ix', 'j', 'ja', 'jh', 'ji', 'js', 'jv', 'k', 'ka', 'ke',\n",
       "        'kg', 'kh', 'ki', 'kj', 'km', 'ko', 'kr', 'ks', 'kt', 'ku', 'ky',\n",
       "        'l', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'li', 'lk', 'll', 'lm',\n",
       "        'ln', 'lo', 'lp', 'ls', 'lt', 'lu', 'ly', 'ma', 'mb', 'mc', 'md',\n",
       "        'me', 'mf', 'mi', 'ml', 'mm', 'mn', 'mo', 'mp', 'mr', 'ms', 'mt',\n",
       "        'mu', 'mw', 'my', 'n', 'na', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh',\n",
       "        'ni', 'nk', 'nn', 'no', 'ns', 'nt', 'nv', 'ny', 'oa', 'ob', 'oc',\n",
       "        'od', 'of', 'og', 'oh', 'oi', 'ok', 'ol', 'om', 'on', 'op', 'or',\n",
       "        'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'p', 'pa', 'pb',\n",
       "        'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pm', 'pp', 'pr', 'ps',\n",
       "        'pt', 'py', 'pz', 'q', 'qd', 'qi', 'qu', 'r', 'ra', 'rb', 'rc',\n",
       "        'rd', 're', 'rf', 'rg', 'ri', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp',\n",
       "        'rr', 'rs', 'rt', 'ru', 'ry', 'sa', 'sb', 'sc', 'sd', 'se', 'sf',\n",
       "        'sg', 'sh', 'si', 'sk', 'sm', 'so', 'sp', 'sr', 'ss', 'st', 'su',\n",
       "        'sw', 'sy', 'ta', 'tc', 'td', 'te', 'tf', 'th', 'ti', 'tl', 'tm',\n",
       "        'to', 'tp', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'ty', 'tz', 'u',\n",
       "        'ua', 'ub', 'uc', 'ud', 'ue', 'ug', 'uk', 'ul', 'um', 'un', 'up',\n",
       "        'ur', 'us', 'ut', 'uu', 'ux', 'uy', 'uz', 'v', 'va', 've', 'vi',\n",
       "        'vm', 'vo', 'vu', 'vy', 'wa', 'wd', 'we', 'wg', 'wi', 'wn', 'wo',\n",
       "        'wp', 'wr', 'ws', 'wt', 'ww', 'x', 'xb', 'xi', 'xt', 'xx', 'ya',\n",
       "        'yb', 'ye', 'yi', 'yl', 'yo', 'ys', 'yu', 'yz', 'z', 'za', 'ze',\n",
       "        'zi', 'zl', 'zs', 'zu'], dtype=object),\n",
       " array(['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'ak', 'al',\n",
       "        'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw',\n",
       "        'ax', 'ay', 'az', 'b', 'ba', 'bc', 'bd', 'be', 'bi', 'bm', 'bn',\n",
       "        'bo', 'br', 'bs', 'bt', 'by', 'c', 'ca', 'cb', 'cc', 'cd', 'ce',\n",
       "        'cf', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cr',\n",
       "        'cs', 'ct', 'cu', 'cw', 'cy', 'cz', 'da', 'db', 'dc', 'dd', 'de',\n",
       "        'df', 'dg', 'dh', 'di', 'dm', 'dn', 'do', 'dp', 'dr', 'ds', 'dt',\n",
       "        'du', 'dy', 'e', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh',\n",
       "        'ei', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'er', 'es', 'et', 'eu',\n",
       "        'ev', 'ew', 'ex', 'ey', 'ez', 'f', 'fa', 'fe', 'ff', 'fi', 'fp',\n",
       "        'fr', 'fs', 'ft', 'fv', 'fy', 'g', 'ga', 'gb', 'gd', 'ge', 'gh',\n",
       "        'gi', 'gm', 'gn', 'go', 'gr', 'gs', 'gu', 'gy', 'h', 'ha', 'hb',\n",
       "        'hd', 'he', 'hg', 'hi', 'hm', 'ho', 'hr', 'hs', 'ht', 'hu', 'hy',\n",
       "        'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'ig', 'ii', 'ik', 'il', 'im',\n",
       "        'in', 'io', 'ip', 'ir', 'is', 'it', 'iv', 'ix', 'iz', 'j', 'ja',\n",
       "        'ji', 'k', 'ka', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'km', 'ko',\n",
       "        'ks', 'ky', 'l', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'li', 'lk',\n",
       "        'll', 'lm', 'lo', 'lp', 'ls', 'lt', 'ly', 'ma', 'mb', 'mc', 'me',\n",
       "        'mf', 'mg', 'mi', 'ml', 'mm', 'mn', 'mo', 'mp', 'mr', 'ms', 'mt',\n",
       "        'my', 'n', 'na', 'nc', 'nd', 'ne', 'nf', 'ng', 'ni', 'nk', 'nl',\n",
       "        'nn', 'no', 'ns', 'nt', 'nu', 'nv', 'ny', 'nz', 'oa', 'ob', 'oc',\n",
       "        'od', 'oe', 'of', 'og', 'oi', 'ok', 'ol', 'om', 'on', 'oo', 'op',\n",
       "        'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'p', 'pa', 'pc',\n",
       "        'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pl', 'pm', 'po', 'pp', 'pr',\n",
       "        'ps', 'pt', 'pu', 'pv', 'pw', 'py', 'q', 'qa', 'qd', 'qi', 'qu',\n",
       "        'r', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'ri', 'rk', 'rl',\n",
       "        'rm', 'rn', 'ro', 'rp', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'ry',\n",
       "        'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sk', 'sl',\n",
       "        'sm', 'so', 'sp', 'sr', 'ss', 'st', 'sy', 'ta', 'tb', 'tc', 'td',\n",
       "        'te', 'tf', 'th', 'ti', 'tk', 'tm', 'tn', 'to', 'tp', 'tr', 'ts',\n",
       "        'tt', 'tu', 'tv', 'ty', 'tz', 'u', 'ua', 'ub', 'uc', 'ud', 'ue',\n",
       "        'uf', 'ug', 'ui', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'ur', 'us',\n",
       "        'ut', 'uu', 'ux', 'uy', 'uz', 'v', 'va', 'vc', 've', 'vi', 'vm',\n",
       "        'vn', 'vo', 'vs', 'vu', 'vy', 'w', 'wa', 'wd', 'we', 'wf', 'wg',\n",
       "        'wi', 'wn', 'wo', 'wp', 'wr', 'ws', 'ww', 'x', 'xb', 'xh', 'xi',\n",
       "        'xo', 'xt', 'xv', 'xx', 'ya', 'ye', 'yi', 'ym', 'yo', 'yp', 'yr',\n",
       "        'ys', 'yy', 'yz', 'za', 'ze', 'zi', 'zn', 'zo'], dtype=object)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "723754cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2621484/4188385426.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  col=[np.array(list(zip(([enc.categories_.index(arr)+1]*len(arr)),list(arr)))) for arr in enc.categories_]\n"
     ]
    }
   ],
   "source": [
    "col=[np.array(list(zip(([enc.categories_.index(arr)+1]*len(arr)),list(arr)))) for arr in enc.categories_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebefb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "co=np.concatenate(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "384086e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', 'aa'],\n",
       "       ['1', 'ab'],\n",
       "       ['1', 'ac'],\n",
       "       ...,\n",
       "       ['4', 'zi'],\n",
       "       ['4', 'zn'],\n",
       "       ['4', 'zo']], dtype='<U21')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c74f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "co=np.concatenate([co,[['target','target']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "364b803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1579"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c57185e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1579)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399cbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#First we transform each df in the following array: first element= last two letters of the first word\n",
    "                                                    #second elt = last two letters of the second word\n",
    "                                                    #third elt= last two letters of the fourth word\n",
    "                                                    #fourth elt = last two letters of the 5th word\n",
    "                                                    #fifth element = POS of the third word\n",
    "def transform(all_samples):\n",
    "    trans=[]\n",
    "    for df in all_samples:\n",
    "        arr=np.zeros(5,dtype=object)\n",
    "        arr[0]=df['word'].iloc[0][-2:]\n",
    "        arr[1]=df['word'].iloc[1][-2:]\n",
    "        arr[2]=df['word'].iloc[3][-2:]\n",
    "        arr[3]=df['word'].iloc[4][-2:]\n",
    "        arr[4]=df['POS'].iloc[2]\n",
    "        trans.append(arr)\n",
    "    return trans\n",
    "\n",
    "def create_df(all_samples):\n",
    "\n",
    "    transformed=transform(all_samples)\n",
    "\n",
    "    #We join them in one matrix:\n",
    "    matrice=np.stack(transformed)\n",
    "\n",
    "    #We chose a class to classify: VBZ\n",
    "    matrice[:,4]=1*(matrice[:,4]=='NN')\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    m=enc.fit_transform(matrice[:,:4]).toarray()\n",
    "\n",
    "    m_final=np.concatenate((m,matrice[:,4][:,None]),axis=1)\n",
    "    col=[np.array(list(zip(([enc.categories_.index(arr)+1]*len(arr)),list(arr)))) for arr in enc.categories_]\n",
    "    co=np.concatenate(col)\n",
    "    co=np.concatenate([co,[['target','target']]])\n",
    "    df=pd.DataFrame(m_final,columns=co)\n",
    "    df=df.rename(columns={('target', 'target'):'target'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "10a34697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, aa)               1.0\n",
       "(1, ab)              16.0\n",
       "(1, ac)              23.0\n",
       "(1, ad)             142.0\n",
       "(1, ae)               2.0\n",
       "                    ...  \n",
       "(4, ze)              30.0\n",
       "(4, zi)               3.0\n",
       "(4, zn)               1.0\n",
       "(4, zo)               1.0\n",
       "(target, target)    14152\n",
       "Length: 1579, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4ead30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={('target', 'target'):'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "daa55700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, ht)    1.0\n",
       "(2, od)    1.0\n",
       "(3, es)    1.0\n",
       "(4, ba)    1.0\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5,:][df.iloc[5,:]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f6bc19b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>examined</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cases</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cuba</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  POS\n",
       "2     right   NN\n",
       "3      food   NN\n",
       "4  examined  VBN\n",
       "5     cases  NNS\n",
       "6      cuba  NNP"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ca21cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "m=enc.fit_transform(mat[:,:4]).toarray()\n",
    "m_final=np.concatenate((m,mat[:,4][:,None]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef435923",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         codes, uniques = factorize_array(\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     uniques, codes = table.factorize(\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2547605/357825308.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all arrays must be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize_from_iterables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterables\u001b[0;34m(iterables)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m     \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2782\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m     \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2782\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2752\u001b[0m         \u001b[0;31m# but only the resulting categories, the order of which is independent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m         \u001b[0;31m# from ordered. Set ordered to False as default. See GH #15457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2754\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2755\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;31m# raise, as we don't have a sortable data structure and so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0mna_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         codes, uniques = factorize_array(\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     uniques, codes = table.factorize(\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     )\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "pd.MultiIndex.from_arrays(zip([1,2,3,4],enc.categories_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6103895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f4818ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (50000, 1605), indices imply (50000, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2547605/1728265750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (50000, 1605), indices imply (50000, 5)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_final=pd.DataFrame(m_final,columns=enc.categories_+['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fa554b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96df48bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0],\n",
       "       [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0],\n",
       "       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "        1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0],\n",
       "       [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
       "        0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0],\n",
       "       [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n",
       "        0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0]], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((m,mat[:,4][:,None]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa34871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['al', 'le', 'ne', 'nt', 'ws'], dtype=object),\n",
       " array(['il', 'ns', 'on', 'ts'], dtype=object),\n",
       " array(['ar', 'ce', 'de', 'es', 'nt'], dtype=object),\n",
       " array(['al', 'ar', 'eb', 'nt', 'om'], dtype=object),\n",
       " array([0], dtype=object)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c7db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a df with one column for each unique combination of each of the 4 first columns of matrice\n",
    "list1=list(set(list(matrice[:,0])))\n",
    "list2=list(set(list(matrice[:,1])))\n",
    "list3=list(set(list(matrice[:,2])))\n",
    "list4=list(set(list(matrice[:,3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa23982b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]*len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f87b0f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>examined</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cases</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cuba</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  POS\n",
       "2     right   NN\n",
       "3      food   NN\n",
       "4  examined  VBN\n",
       "5     cases  NNS\n",
       "6      cuba  NNP"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fdc594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.zeros(5, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a7327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]='re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77b85ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2728187/2058053819.py:34: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  col=[np.array(list(zip(([enc.categories_.index(arr)+1]*len(arr)),list(arr)))) for arr in enc.categories_]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, ab)</th>\n",
       "      <th>(1, ac)</th>\n",
       "      <th>(1, ad)</th>\n",
       "      <th>(1, ae)</th>\n",
       "      <th>(1, af)</th>\n",
       "      <th>(1, ag)</th>\n",
       "      <th>(1, ah)</th>\n",
       "      <th>(1, ai)</th>\n",
       "      <th>(1, ak)</th>\n",
       "      <th>(1, al)</th>\n",
       "      <th>...</th>\n",
       "      <th>(4, yi)</th>\n",
       "      <th>(4, yl)</th>\n",
       "      <th>(4, yo)</th>\n",
       "      <th>(4, yp)</th>\n",
       "      <th>(4, ys)</th>\n",
       "      <th>(4, yz)</th>\n",
       "      <th>(4, za)</th>\n",
       "      <th>(4, ze)</th>\n",
       "      <th>(4, zi)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1596 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      (1, ab) (1, ac) (1, ad) (1, ae) (1, af) (1, ag) (1, ah) (1, ai) (1, ak)  \\\n",
       "25000     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25001     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25002     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25003     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25004     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25005     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25006     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25007     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25008     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25009     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      (1, al)  ... (4, yi) (4, yl) (4, yo) (4, yp) (4, ys) (4, yz) (4, za)  \\\n",
       "25000     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25001     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25002     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25003     1.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25004     0.0  ...     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "25005     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25006     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25007     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25008     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25009     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      (4, ze) (4, zi) target  \n",
       "25000     0.0     0.0      0  \n",
       "25001     0.0     0.0      0  \n",
       "25002     0.0     0.0      0  \n",
       "25003     0.0     0.0      0  \n",
       "25004     0.0     0.0      0  \n",
       "25005     0.0     0.0      1  \n",
       "25006     0.0     0.0      0  \n",
       "25007     0.0     0.0      1  \n",
       "25008     0.0     0.0      1  \n",
       "25009     0.0     0.0      0  \n",
       "\n",
       "[10 rows x 1596 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import random\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "fulldf = create_df(all_samples)\n",
    "fulldf[25000:25010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9fbd67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2728187/1893345464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25010\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lt2222-v22-a2/mycode.py\u001b[0m in \u001b[0;36mcreate_df\u001b[0;34m(all_samples)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtransformed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m#We join them in one matrix:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lt2222-v22-a2/mycode.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(all_samples)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "fulldf = mc.create_df(all_samples)\n",
    "fulldf[25000:25010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b87ed",
   "metadata": {},
   "source": [
    "## Part 4 - extract training and testing sets (3 points)\n",
    "\n",
    "Here, you will create the training and testing datasets in order to train the model.  This will be based on a test percentage.  Round up if the percentage does not divide evenly into the sample size.  You will need to separate the class column into the y-values for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1eeaf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "49995    0\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    1\n",
       "Name: target, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldf.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6493d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_samples(fulldf, test_percent):\n",
    "    X=fulldf.iloc[:,:-1]\n",
    "    y=fulldf.iloc[:,-1]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_percent/100)\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02fda612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 10000, 10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = split_samples(fulldf, test_percent=20)\n",
    "len(train_X), len(train_y), len(test_X), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc332791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = mc.split_samples(fulldf, test_percent=20)\n",
    "len(train_X), len(train_y), len(test_X), len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160744cb",
   "metadata": {},
   "source": [
    "## Part 5 - train models (3 points)\n",
    "\n",
    "You will then train and return two support vector machine (SVM) models using the sklearn SVC class.  You should allow a choice between linear and radial basis function kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f53eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=train_y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "031aac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def train(train_X, train_y, kernel):\n",
    "    clf=SVC(kernel=kernel)\n",
    "    clf.fit(train_X, train_y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = train(train_X, train_y, kernel='linear')\n",
    "model_rbf = train(train_X, train_y, kernel=\"rbf\")\n",
    "model_linear, model_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eedaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = mc.train(train_X, train_y, kernel='linear')\n",
    "model_rbf = mc.train(train_X, train_y, kernel=\"rbf\")\n",
    "model_linear, model_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d4664",
   "metadata": {},
   "source": [
    "## Part 6 - evaluate the models (5 points)\n",
    "\n",
    "You will calculate and print precision, recall, and F-measure for the models on the test data. In `notes.md`, write down your comparison of these simple measures on the two models and any thoughts you might have on what they mean. (It could be very short, and since the samples do not stay stable between runs, you can save the evaluation scores in `notes.md` too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff853e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def eval_model(model_linear, test_X, test_y):\n",
    "    y_pred=model_linear.predict(test_X)\n",
    "    prec,recall,fscore,_=precision_recall_fscore_support(test_y,y_pred)\n",
    "    return prec,recall,fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a48c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model_linear, test_X, test_y)\n",
    "eval_model(model_rbf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.eval_model(model_linear, test_X, test_y)\n",
    "mc.eval_model(model_rbf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9b71e",
   "metadata": {},
   "source": [
    "## Part Bonus - try another sort of model from sklearn (5 points)\n",
    "\n",
    "Write a separate, command-line script (not a notebook) uses `mycode.py` to do all of the above, except that it trains a non-SVM classifier model.  Any non-trivial model available in sklearn will do. Explain how to run your code and the results of your own evaluation in `notes.md`, including any observations or opinions you may have on the classifier method you used in comparison to SVM.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Push to your fork of the GitHub repository (which must be made public) and submit the URL of your repository in Canvas.  You can submit this notebook with the output from your run, as long as you do not modify the code or text in it without permission from us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def eval_Naive_Bayes(train_X, train_y, test_X, test_y):\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    prec,recall,fscore,_=precision_recall_fscore_support(test_y,y_pred)\n",
    "    return prec,recall,fscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
